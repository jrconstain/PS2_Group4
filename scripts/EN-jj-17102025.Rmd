---
title: "EN"
author: "G4"
date: "2025-10-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)

p_load(rio, # Import/export data.
       tidyverse, # Tidy-data.
       stargazer, # Descriptive statistics.
       gt, # Descriptive statistics.
       gtsummary,
       caret, # For predictive model assessment.
       doParallel, 
       parallel,
       gridExtra, # Arrange plots.
       skimr, # Summarize data.
       here, #for file searching
       readr, #for opening files
       ggplot2,
       boot,
       scales,
       car,
       dplyr,
       tidyr,
       plurrr,
       MLmetrics, 
       glmnet
       )
```

```{r}
# Carga de datos

ruta7 <- here("stores", "train_hogares_enriquecido_final.csv")
train_use <- read_csv(ruta7)

ruta8 <- here("stores", "test_hogares_enriquecido_final.csv")
test_final <- read_csv(ruta8)

```
```{r}

# ============= Preparar Paralelización ===============

# Detectar el número de núcleos disponibles
n_cores <- detectCores() - 1  # deja 1 libre para el sistema

# Crear el clúster
cl <- makeCluster(n_cores)

# Registrar el clúster para que caret lo use
registerDoParallel(cl)

# Confirmar
getDoParWorkers()

```


```{r}
# ============================================================
# Modelo Elastic Net con variables finales completas
# Mismo: Modelo 4 (Logit + EN) 0.65
# ============================================================

train_use <- droplevels(train_use)

fiveStats <- function(...) c(prSummary(...))

ctrl <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = fiveStats,
  savePredictions = TRUE,
  allowParallel = TRUE
)

cat("Entrenando modelo con", getDoParWorkers(), "núcleos...\n")
start_time <- Sys.time()

tryCatch({
  set.seed(2050)
  modelo_EN_final <- train(
    Pobre ~ . , 
    data = train_use,
    metric = "F",
    method = "glmnet",
    family = "binomial",
    trControl = ctrl,
    tuneGrid = expand.grid(
      alpha  = seq(0, 1, by = 0.1),
      lambda = 10^seq(-3, 3, length = 10)
    )
  )
}, finally = {
  # Guardar siempre aunque falle
  if (exists("modelo_EN_final")) {
    saveRDS(modelo_EN_final, "modelo_EN_checkpoint_final.rds")
  }
  end_time <- Sys.time()
  cat("Entrenamiento finalizado o interrumpido tras",
      round(difftime(end_time, start_time, units = "mins"), 2),
      "minutos\n")
})

stopCluster(cl)
registerDoSEQ()

```

```{r}
# ============================================================
# Seleccionar el mejor Threshold interno
# ============================================================

preds <- modelo_EN_1$pred
umbrales <- seq(0.1, 0.9, by = 0.05)

# Seleccionar el mejor alpha y lambda
best_alpha  <- modelo_EN_1$bestTune$alpha
best_lambda <- modelo_EN_1$bestTune$lambda

# Filtrar solo las predicciones del mejor modelo
preds_best <- preds %>%
  dplyr::filter(alpha == best_alpha, lambda == best_lambda)


f1_scores <- sapply(umbrales, function(th) {
  pred_class <- ifelse(preds_best$pobre >= th, "pobre", "no_pobre")
  F_meas(as.factor(pred_class), as.factor(preds_best$obs), relevant = "pobre")
})


plot(umbrales, f1_scores, type = "b", pch = 19,
     main = "F1-score vs Umbral",
     xlab = "Umbral de decisión", ylab = "F1-score")

best_th <- umbrales[which.max(f1_scores)]
best_th

```

```{r}

# ============================================================
# Predicciones y exportación (Elastic Net) 
# ============================================================

pred_prob <- predict(modelo_EN_1, newdata = test_final, type = "prob")
prob_1 <- pred_prob[, 2]
pred_01 <- as.integer(prob_1 >= best_th)

# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogares <- tibble(
  id = id_out,
  prob_pobre_1 = prob_1,
  pobre_pred   = pred_01
)

# Mantener consistencia con nombres y formato
best_alpha  <- modelo_EN_1$bestTune$alpha
best_lambda <- modelo_EN_1$bestTune$lambda

nombre_archivo <- sprintf(
  "EN_lambda_%s_alpha_%s_th_%s.csv",
  format(round(best_lambda, 3), scientific = FALSE),
  format(round(best_alpha, 2), nsmall = 2),
  format(round(0.35, 2), nsmall = 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogares %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivo))
```
