---
title: "logit"
author: "G4"
date: "2025-10-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)

p_load(rio, # Import/export data.
       tidyverse, # Tidy-data.
       stargazer, # Descriptive statistics.
       gt, # Descriptive statistics.
       gtsummary,
       caret, # For predictive model assessment.
       gridExtra, # Arrange plots.
       skimr, # Summarize data.
       here, #for file searching
       readr, #for opening files
       ggplot2,
       boot,
       scales,
       car,
       dplyr,
       tidyr,
       plurrr,
       MLmetrics, 
       glmnet,
       paralell,
       doParallel
       )
```

```{r}
ruta1 <- here("stores", "train_hogares.csv")
df_train_hogares <- read_csv(ruta1)

ruta2 <- here("stores", "train_personas.csv")
df_train_personas <- read_csv(ruta2)

ruta3 <- here("stores", "test_personas.csv")
df_test_personas <- read_csv(ruta3)

ruta4 <- here("stores", "test_hogares.csv")
df_test_hogares <- read_csv(ruta4)
```

```{r}

# modelo 1 (Logit con variables iniciales household) 0.36
set.seed(1011)

# variables a usar
vars <- c("P5000","P5010","P5090","P5100","P5130","P5140",
          "Nper","Npersug","Depto","Lp")

# Armar train/test
train_use <- df_train_hogares %>%
  select(Pobre, all_of(vars)) %>%
  mutate(Pobre = factor(Pobre, levels = c(0,1), labels = c("no_pobre","pobre")))

test_final <- df_test_hogares %>%
  select(all_of(vars))

# Convertir factores
if ("Depto" %in% vars) {
  train_use$Depto <- factor(train_use$Depto)
  test_final$Depto <- as.character(test_final$Depto)
}
if ("P5090" %in% vars) {
  train_use$P5090 <- factor(train_use$P5090)
  test_final$P5090 <- as.character(test_final$P5090)
}

# IMPUTACIÓN
# Numéricas: mediana del train
num_vars <- vars[!(vars %in% c("Depto","P5090"))]
for (v in num_vars) {
  med <- median(train_use[[v]], na.rm = TRUE)
  train_use[[v]][is.na(train_use[[v]])] <- med
  test_final[[v]][is.na(test_final[[v]])] <- med
}

# Categóricas: moda del train
if ("Depto" %in% vars) {
  mode_depto <- names(which.max(table(train_use$Depto)))
  train_use$Depto[is.na(train_use$Depto)] <- mode_depto
  test_depto <- test_final$Depto
  test_depto[is.na(test_depto) | !(test_depto %in% levels(train_use$Depto))] <- mode_depto
  test_final$Depto <- factor(test_depto, levels = levels(train_use$Depto))
}
if ("P5090" %in% vars) {
  mode_p5090 <- names(which.max(table(train_use$P5090)))
  train_use$P5090[is.na(train_use$P5090)] <- mode_p5090
  test_p5090 <- test_final$P5090
  test_p5090[is.na(test_p5090) | !(test_p5090 %in% levels(train_use$P5090))] <- mode_p5090
  test_final$P5090 <- factor(test_p5090, levels = levels(train_use$P5090))
}

# control y modelo
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, savePredictions = TRUE)
form <- as.formula(paste("Pobre ~", paste(vars, collapse = " + ")))

set.seed(1410)
modelo_logit <- train(
  form, data = train_use,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  na.action = na.omit
)

# predicción
pred_prob <- predict(modelo_logit, newdata = test_final, type = "prob")

# Umbral
th <- 0.5

# Predicción binaria 0/1 usando la prob. de "pobre"
prob_1 <- pred_prob[,"pobre"]
pred_01 <- as.integer(prob_1 >= th)

# Salida: id, probabilidad y predicción 0/1
id_out <- if ("id" %in% names(df_test_hogares)) df_test_hogares$id else seq_len(nrow(df_test_hogares))

predicciones_hogares <- tibble(
  id = id_out,
  prob_pobre_1 = prob_1,
  pobre_pred   = pred_01
)

# Formato Kaggle
kaggle_sub <- predicciones_hogares %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Ponle un nombre siguiendo la convención
readr::write_csv(kaggle_sub, "LOGIT_treshold_05.csv")

head(kaggle_sub)
```

mean_edu_occ prop_occ_nper prop_occ_pet jefe_max_edu prom_horas_occ

```{r}
# modelo 2 (Logit con primer set de variables individuales agregadas) 0.57

ruta5 <- here("stores", "train_hogares_enriquecido.csv")
train_hogares_enriquecido <- read_csv(ruta5)

ruta6 <- here("stores", "test_hogares_enriquecido.csv")
test_hogares_enriquecido <- read_csv(ruta6)

```

```{r}

set.seed(1011)

# var
vars <- c(
  "P5000","P5010","P5090","P5100","P5130","P5140",
  "Nper","Npersug","Depto","Lp",
  "mean_edu_occ","prop_occ_nper","prop_occ_pet","jefe_max_edu","prom_horas_occ"
)

# Armar train/test
train_use <- train_hogares_enriquecido %>%
  select(Pobre, all_of(vars)) %>%
  mutate(Pobre = factor(Pobre, levels = c(0,1), labels = c("no_pobre","pobre")))

test_final <- test_hogares_enriquecido %>%
  select(all_of(vars))

# solo Depto y P5090 son categóricas (además de Pobre)
cat_vars <- c("Depto","P5090")
num_vars <- setdiff(vars, cat_vars)

# Train: factores; Test: char para mapear y luego fijar niveles
train_use[cat_vars]  <- lapply(train_use[cat_vars],  factor)
test_final[cat_vars] <- lapply(test_final[cat_vars], as.character)

# IMPUTACIÓN (aprendida en TRAIN, aplicada a TRAIN y TEST)
# Numéricas -> mediana del TRAIN
for (v in num_vars) {
  med <- median(train_use[[v]], na.rm = TRUE)
  train_use[[v]][is.na(train_use[[v]])] <- med
  test_final[[v]][is.na(test_final[[v]])] <- med
}

# Categóricas
for (v in cat_vars) {
  mode_v <- names(which.max(table(train_use[[v]])))
  train_use[[v]][is.na(train_use[[v]])] <- mode_v
  tmp <- test_final[[v]]
  tmp[is.na(tmp) | !(tmp %in% levels(train_use[[v]]))] <- mode_v
  test_final[[v]] <- factor(tmp, levels = levels(train_use[[v]]))
}

# 4.3 Drop levels en TRAIN
train_use <- droplevels(train_use)

# 5) caret: control y modelo
ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, savePredictions = TRUE)
form <- as.formula(paste("Pobre ~", paste(vars, collapse = " + ")))

set.seed(1410)
modelo_logit <- train(
  form, data = train_use,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  na.action = na.omit
)

# 6) Predicción en TEST (prob y clase 0/1 con th=0.5)
pred_prob <- predict(modelo_logit, newdata = test_final, type = "prob")
prob_1 <- pred_prob[,"pobre"]
pred_01 <- as.integer(prob_1 >= 0.5)

# 7) Salida con prob y pred
id_out <- if ("id" %in% names(test_hogares_enriquecido)) test_hogares_enriquecido$id else seq_len(nrow(test_hogares_enriquecido))

predicciones_hogares <- tibble(
  id = id_out,
  prob_pobre_1 = prob_1,
  pobre_pred   = pred_01
)

# 8) Formato Kaggle (solo id, pobre)
kaggle_sub <- predicciones_hogares %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

write_csv(kaggle_sub,  here::here("stores", "LOGIT_th_050_V2.csv"))
```

```{r}
# modelo 3 (Logit con segundo set de variables individuales agregadas) 0.62

ruta7 <- here("stores", "train_hogares_enriquecido_2.csv")
train_hogares_enriquecido <- read_csv(ruta7)

ruta8 <- here("stores", "test_hogares_enriquecido_2.csv")
test_hogares_enriquecido <- read_csv(ruta8)

```

```{r}
# ============================================================
# LOGIT con 15 variables + interacciones y potencias 
# ============================================================

set.seed(2050)

# Variables base (las nuevas + básicas de hogar)
vars <- c(
  "P5000","P5010","P5090","P5100","P5130","P5140",
  "Nper","Npersug","Depto","Lp",
  # variables de personas a nivel hogar
  "mean_edu_occ","prop_occ_nper","prop_occ_pet","jefe_max_edu","horas_prom_occ",
  "sexo_jefe","antig_jefe","antig_prom_occ","edu_prom_occ","prop_des",
  "edad_jefe","edad_prom_occ","prestaciones","tam_emp_jefe","tam_emp_prom_occ",
  "cotiza_pens_jefe","pos_ocup_jefe"
)

# Limpia duplicados después de hacer todos los joins
train_hogares_enriquecido <- train_hogares_enriquecido %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  distinct(id, .keep_all = TRUE)

test_hogares_enriquecido <- test_hogares_enriquecido %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  distinct(id, .keep_all = TRUE)

# Mutar
train_hogares_enriquecido <- train_hogares_enriquecido %>%
  mutate(
    edad_jefe2       = edad_jefe^2,
    edad_prom_occ2   = edad_prom_occ^2,
    sexo_desocup     = sexo_jefe * prop_des,
    edu_occ_lp       = mean_edu_occ * Lp,
    horasxedu_occ    = horas_prom_occ * mean_edu_occ   # ← aquí el cambio
  )

test_hogares_enriquecido <- test_hogares_enriquecido %>%
  mutate(
    edad_jefe2       = edad_jefe^2,
    edad_prom_occ2   = edad_prom_occ^2,
    sexo_desocup     = sexo_jefe * prop_des,
    edu_occ_lp       = mean_edu_occ * Lp,
    horasxedu_occ    = horas_prom_occ * mean_edu_occ
  )


# Añadir nuevas al vector de variables
vars <- c(vars, "edad_jefe2", "edad_prom_occ2", "sexo_desocup", "edu_occ_lp", "horasxedu_occ")

# ============================================================
# Armar train/test
# ============================================================

train_use <- train_hogares_enriquecido %>%
  select(Pobre, all_of(vars)) %>%
  mutate(Pobre = factor(Pobre, levels = c(0,1), labels = c("no_pobre","pobre")))

test_final <- test_hogares_enriquecido %>%
  select(all_of(vars))

# Categóricas (igual que antes)
cat_vars <- c("Depto","P5090","pos_ocup_jefe","cotiza_pens_jefe")
num_vars <- setdiff(vars, cat_vars)

train_use[cat_vars]  <- lapply(train_use[cat_vars],  factor)
test_final[cat_vars] <- lapply(test_final[cat_vars], as.character)

# ============================================================
# IMPUTACIÓN (Considerando que este código tiene probemas de imputación identificados)
# ============================================================
#
# Numéricas -> mediana
for (v in num_vars) {
  med <- median(train_use[[v]], na.rm = TRUE)
  train_use[[v]][is.na(train_use[[v]])] <- med
  test_final[[v]][is.na(test_final[[v]])] <- med
}

# Categóricas -> moda
for (v in cat_vars) {
  mode_v <- names(which.max(table(train_use[[v]])))
  train_use[[v]][is.na(train_use[[v]])] <- mode_v
  tmp <- test_final[[v]]
  tmp[is.na(tmp) | !(tmp %in% levels(train_use[[v]]))] <- mode_v
  test_final[[v]] <- factor(tmp, levels = levels(train_use[[v]]))
}
train_use <- droplevels(train_use)

# ============================================================
# Modelo LOGIT
# ============================================================

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, savePredictions = TRUE)
form <- as.formula(paste("Pobre ~", paste(vars, collapse = " + ")))

set.seed(2050)
modelo_logit3 <- train(
  form, data = train_use,
  method = "glm",
  family = "binomial",
  trControl = ctrl,
  na.action = na.omit
)

# ============================================================
# Predicciones
# ============================================================

pred_prob <- predict(modelo_logit3, newdata = test_final, type = "prob")
prob_1 <- pred_prob[,"pobre"]
pred_01 <- as.integer(prob_1 >= 0.5)

id_out <- if ("id" %in% names(test_hogares_enriquecido)) test_hogares_enriquecido$id else seq_len(nrow(test_hogares_enriquecido))

predicciones_hogares <- tibble(
  id = id_out,
  prob_pobre_1 = prob_1,
  pobre_pred   = pred_01
)

kaggle_sub <- predicciones_hogares %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", "LOGIT_th_050_V3.csv"))
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


```{r}
#Modelo 4 (Logit + EN) 0.65

# ============================================================
# Modelo Elastic Net
# ============================================================

fiveStats <- function(...) c(prSummary(...))

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)

set.seed(2050)
modelo_EN_1<- train(
  Pobre ~ . , 
  data = train_use,
  metric = "F",
  method = "glmnet",
  family = "binomial",
  trControl = ctrl,
  tuneGrid = expand.grid(
    alpha  = seq(0, 1, by= 0.1),
    lambda = 10^seq(-3, 3, length = 10)
  )   
)

```

```{r}
# ============================================================
# Seleccionar el mejor Threshold interno
# ============================================================

preds <- modelo_EN_1$pred
umbrales <- seq(0.1, 0.9, by = 0.05)

# Seleccionar el mejor alpha y lambda
best_alpha  <- modelo_EN_1$bestTune$alpha
best_lambda <- modelo_EN_1$bestTune$lambda

# Filtrar solo las predicciones del mejor modelo
preds_best <- preds %>%
  dplyr::filter(alpha == best_alpha, lambda == best_lambda)


f1_scores <- sapply(umbrales, function(th) {
  pred_class <- ifelse(preds_best$pobre >= th, "pobre", "no_pobre")
  F_meas(as.factor(pred_class), as.factor(preds_best$obs), relevant = "pobre")
})


plot(umbrales, f1_scores, type = "b", pch = 19,
     main = "F1-score vs Umbral",
     xlab = "Umbral de decisión", ylab = "F1-score")

best_th <- umbrales[which.max(f1_scores)]
best_th

```

```{r}

# ============================================================
# Predicciones y exportación (Elastic Net) 
# ============================================================

pred_prob <- predict(modelo_EN_1, newdata = test_final, type = "prob")
prob_1 <- pred_prob[, 2]
pred_01 <- as.integer(prob_1 >= 0.35)

# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogares <- tibble(
  id = id_out,
  prob_pobre_1 = prob_1,
  pobre_pred   = pred_01
)

# Mantener consistencia con nombres y formato
best_alpha  <- modelo_EN_1$bestTune$alpha
best_lambda <- modelo_EN_1$bestTune$lambda

nombre_archivo <- sprintf(
  "EN_lambda_%s_alpha_%s_th_%s.csv",
  format(round(best_lambda, 3), scientific = FALSE),
  format(round(best_alpha, 2), nsmall = 2),
  format(round(0.35, 2), nsmall = 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogares %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivo))
```

```{r}
#Modelo 5 (Random Forest) 0.69

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)


set.seed(2050)  # Fijamos semilla para reproducibilidad

p_load("ranger")
tree_ranger_grid <-train (
    Pobre ~ . ,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl,  # Especificamos los controles de entrenamiento
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 200
)
tree_ranger_grid


test_RF<- test_final  %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "raw"))
probRF_1 <- ifelse (test_RF$pobre_hat_ranger=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- tree_ranger_grid$bestTune$splitrule

nombre_archivoRF <- sprintf(
  "RF_mtry_%s_min.node.size_%s_SR_%s.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))

###Modelo 7 (Random Forest - Optimizamos threshold con Precision recall Curve)  0.68

p_load("pROC") # Paquete para calcular y visualizar curvas ROC

roc_obj_en<-roc(response=tree_ranger_grid$pred$obs,  # Valores reales de la variable objetivo
                   predictor=tree_ranger_grid$pred$pobre, # Probabilidades predichas por el modelo
                  levels = c("no_pobre", "pobre"),  # # Establece la referencia control y caso (empleado = negativo, desempleado = positivo) 
                  direction = "<")  # "<" significa que "desempleado" es positivo
     
prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))
prec_recall

prec_recall<- prec_recall  %>% mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

# Predicción de probabilidad de desempleo
test_RFPR<- test_final  %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "prob"))

# Clasificación según el umbral óptimo de F1
test_RFPR <- test_RFPR %>%
  mutate(pobre_hat_rangerPRs_cutoff_F1 = factor(
    ifelse(pobre_hat_ranger$pobre >= prec_recall$threshold[which.max(prec_recall$F1)], 1, 0),
    levels = c(1, 0),
    labels = c("pobre", "no_pobre")
  ))
probRF_1 <- ifelse (test_RFPR$pobre_hat_rangerPRs_cutoff_F1=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- prec_recall$threshold[which.max(prec_recall$F1)]

nombre_archivoRF <- sprintf(
  "RF_mtry_%s_min.node.size_%s_SR_%s.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))



```


```{r}
#Modelo 6 (Random Forest) #IGNORAR 0.65
ctrl <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)


set.seed(2050)  # Fijamos semilla para reproducibilidad

p_load("ranger")
tree_ranger_grid <-train (
    Pobre ~ . ,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl,  # Especificamos los controles de entrenamiento
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(15,30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 500
)
tree_ranger_grid


test_RF<- test_final  %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "raw"))
probRF_1 <- ifelse (test_RF$pobre_hat_ranger=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- tree_ranger_grid$bestTune$splitrule

nombre_archivoRF <- sprintf(
  "RF_mtry_%s_min.node.size_%s_SR_%s.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))



```

```{r}


###Modelo 8 (Random Forest - Optimizamos threshold con Precision recall Curve - imputación correcta) - 0.69

##IMPUTACION CORRECTA
# Numéricas_0 -> mediana
for (v in num_vars) {
  train_use[[v]][is.na(train_use[[v]])] <- 0
  test_final[[v]][is.na(test_final[[v]])] <- 0
}

# cotiza_pens_jefe: imputar NA a "2" (No)
train_use$cotiza_pens_jefe[is.na(train_use$cotiza_pens_jefe)] <- "2"
test_final$cotiza_pens_jefe[is.na(test_final$cotiza_pens_jefe)] <- "2"

# ppos_ocup_jefe: crear nuevo nivel "0" y asignar a NA
levels(train_use$pos_ocup_jefe) <- c(levels(train_use$pos_ocup_jefe), "0")
levels(test_final$pos_ocup_jefe) <- c(levels(test_final$pos_ocup_jefe), "0")

train_use$pos_ocup_jefe[is.na(train_use$pos_ocup_jefe)] <- "0"
test_final$pos_ocup_jefe[is.na(test_final$pos_ocup_jefe)] <- "0"

train_use <- droplevels(train_use)

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)


set.seed(2050)  # Fijamos semilla para reproducibilidad

p_load("ranger")
tree_ranger_grid <-train (
    Pobre ~ . ,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl,  # Especificamos los controles de entrenamiento
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 200
)
tree_ranger_grid


hola=varImp(tree_ranger_grid)

p_load("pROC") # Paquete para calcular y visualizar curvas ROC

roc_obj_en<-roc(response=tree_ranger_grid$pred$obs,  # Valores reales de la variable objetivo
                   predictor=tree_ranger_grid$pred$pobre, # Probabilidades predichas por el modelo
                  levels = c("no_pobre", "pobre"),  # # Establece la referencia control y caso (empleado = negativo, desempleado = positivo) 
                  direction = "<")  # "<" significa que "desempleado" es positivo
     
prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))
prec_recall

prec_recall<- prec_recall  %>% mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

# Predicción de probabilidad de desempleo
test_RFPR<- test_final  %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "prob"))

# Clasificación según el umbral óptimo de F1
test_RFPR <- test_RFPR %>%
  mutate(pobre_hat_rangerPRs_cutoff_F1 = factor(
    ifelse(pobre_hat_ranger$pobre >= prec_recall$threshold[which.max(prec_recall$F1)], 1, 0),
    levels = c(1, 0),
    labels = c("pobre", "no_pobre")
  ))
probRF_1 <- ifelse (test_RFPR$pobre_hat_rangerPRs_cutoff_F1=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- prec_recall$threshold[which.max(prec_recall$F1)]

nombre_archivoRF <- sprintf(
  "RF_mtry_%s_min.node.size_%s_SR_%s2.0.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))

```

```{r}

####MODELO 9 (RF CON IMPUTACIONES CORRECTAS - MAS VARIABLES. THRESHOLD OPTIMIZATION) 0.71

ruta1 <- here("stores", "train_hogares.csv")
df_train_hogares <- read_csv(ruta1)

ruta2 <- here("stores", "train_personas.csv")
df_train_personas <- read_csv(ruta2)

ruta3 <- here("stores", "test_personas.csv")
df_test_personas <- read_csv(ruta3)

ruta4 <- here("stores", "test_hogares.csv")
df_test_hogares <- read_csv(ruta4)

# Create new variables at individual level
# Function to create years of education
add_edu_years <- function(df) {
  df$edu_years <- ifelse(
    df$P6210 %in% c(1, 2, 9), 0, # None, Preschool, Unknown → 0
    ifelse(
      df$P6210 %in% c(3, 4, 5), df$P6210s1, # Primary or Secondary → as is
      ifelse(df$P6210 == 6, df$P6210s1 + 11, NA_real_) # Tertiary → add 11
    )
  )
  df
}


# Function to create years of experience (Mincer-style proxy)
# exp = age - edu_years - 6
add_experience <- function(df) {
  df$exp <- df$P6040 - df$edu_years - 6
  df
}

# Function to Manage NA's on labor indicators
fill_labor_nas <- function(df) {
  vars <- c("Pet", "Oc", "Des", "Ina")
  df %>%
    mutate(across(all_of(vars), ~ as.integer(replace(., is.na(.), 0L))))
}

# Define PET_flexible (Urbano: 15–64 años; Rural: 12–69 años)
add_pet_flexible <- function(df) {
  df$PET_flexible <- ifelse(
    is.na(df$P6040), NA_integer_,
    ifelse(
      df$Clase == 1,                           # Urbano
      ifelse(df$P6040 >= 15 & df$P6040 <= 64, 1L, 0L),
      ifelse(df$P6040 >= 12 & df$P6040 <= 69, 1L, 0L) # Rural
    )
  )
  df
}


# Apply functiona to Train and Test
df_train_personas <- add_edu_years(df_train_personas)
df_test_personas  <- add_edu_years(df_test_personas)

df_train_personas <- add_experience(df_train_personas)
df_test_personas  <- add_experience(df_test_personas)

df_train_personas <- fill_labor_nas(df_train_personas)
df_test_personas  <- fill_labor_nas(df_test_personas)

df_train_personas <- add_pet_flexible(df_train_personas)
df_test_personas  <- add_pet_flexible(df_test_personas)

```


# Agreggate variables to household level

```{r}
agg_personas_to_hogares_plus_2 <- function(df_personas) {

  # Helpers para robustez numérica
  safe_div <- function(a, b) ifelse(is.na(b) | b <= 0, 0, a / b)

  df_personas %>%
    group_by(id) %>%
    summarise(
      # ===== BASE DE CONTEOS ===================================================
      n_personas = n(),
      n_occ      = sum(Oc  == 1, na.rm = TRUE),
      n_des      = sum(Des == 1, na.rm = TRUE),
      n_ina      = sum(Ina == 1, na.rm = TRUE),
      n_pet      = sum(Pet == 1, na.rm = TRUE),

      # ===== PET flexible ===
      n_pet_flex        = sum(PET_flexible == 1, na.rm = TRUE),
      n_no_pet_flex     = n_personas - n_pet_flex,
      n_occ_in_pet_flex = sum(Oc == 1 & PET_flexible == 1, na.rm = TRUE),
      

      # ===== COMPOSICIÓN / PROPORCIONES (LEGADO + NUEVAS) =====================
      prop_occ_nper     = safe_div(n_occ, n_personas),
      prop_des_nper     = safe_div(n_des, n_personas),         # NUEVA
      prop_ina_nper     = safe_div(n_ina, n_personas),         # NUEVA
      prop_no_pet_nper  = safe_div(n_no_pet_flex, n_personas), # NUEVA (PET flexible)
      prop_occ_pet      = safe_div(n_occ, n_pet),              # LEGADO (PET original)
      prop_occ_pet_flex = safe_div(n_occ_in_pet_flex, n_pet_flex), # NUEVA (PET flexible)

      # ===== JEFE DEL HOGAR: CARACTERÍSTICAS ==================================
      edad_jefe       = dplyr::first(P6040[P6050 == 1]),
      sexo_jefe       = dplyr::first(P6020[P6050 == 1]),
      tam_emp_jefe    = dplyr::coalesce(dplyr::first(P6870[P6050 == 1]), 0),
      cotiza_pens_jefe= dplyr::coalesce(dplyr::first(P6920[P6050 == 1]), 0),
      pos_ocup_jefe   = dplyr::coalesce(dplyr::first(P6430[P6050 == 1]), 0),
      antig_jefe      = dplyr::coalesce(dplyr::first(P6426[P6050 == 1]), 0), # si NA -> 0
      jefe_max_edu    = max(P6210[P6050 == 1], na.rm = TRUE),  # LEGADO (cód. educ. máx.)
      jefe_max_edu_years = {
        val <- max(P6210[P6050 == 1], na.rm = TRUE)
        if (is.finite(val)) val else 0
      },
      
      # Indicadores del jefe
      jefe_ocupado     = as.integer(any(P6050 == 1 & Oc == 1, na.rm = TRUE)),
      prestaciones_jefe= as.integer(any(P6050 == 1 &
                                  (P6510 == 1 | P6545 == 1 | P6580 == 1 |
                                   P6630s1 == 1 | P6630s2 == 1 | P6630s3 == 1 |
                                   P6630s4 == 1 | P6630s6 == 1),
                                  na.rm = TRUE)),
      horas_jefe       = ifelse(any(P6050 == 1 & Oc == 1, na.rm = TRUE),
                                dplyr::first(P6800[P6050 == 1 & Oc == 1]), 0),

      # ===== CALIDAD DEL EMPLEO (HOGAR) =======================================
      prestaciones = as.integer(
        any(P6510 == 1, na.rm = TRUE) |
        any(P6545 == 1, na.rm = TRUE) |
        any(P6580 == 1, na.rm = TRUE) |
        any(P6630s1 == 1, na.rm = TRUE) |
        any(P6630s2 == 1, na.rm = TRUE) |
        any(P6630s3 == 1, na.rm = TRUE) |
        any(P6630s4 == 1, na.rm = TRUE) |
        any(P6630s6 == 1, na.rm = TRUE)   # (prioriza versión LEGADA)
      ),
      
      prop_prest_occ = if (n_occ > 0) { 
        mean( 
          (P6510 == 1 | P6545 == 1 | P6580 == 1 | P6630s1 == 1 | P6630s2 == 1 | P6630s3 == 1 | P6630s4 == 1 | P6630s6 == 1)[Oc == 1], 
          na.rm = TRUE 
          ) 
        } else 0,

      # ===== HORAS Y TAMAÑOS (ENTRE OCUPADOS) =================================
      prom_horas_occ  = ifelse(n_occ > 0, mean(P6800[Oc == 1], na.rm = TRUE), 0), # alias LEGADO
      tam_emp_prom_occ= ifelse(n_occ > 0, mean(P6870[Oc == 1], na.rm = TRUE), 0),

      # ===== ANTIGÜEDAD / EDUCACIÓN / EDAD (ENTRE OCUPADOS) ===================
      antig_prom_occ  = ifelse(n_occ > 0, mean(P6426[Oc == 1], na.rm = TRUE), 0),
      mean_edu_occ    = ifelse(n_occ > 0, mean(edu_years[Oc == 1], na.rm = TRUE), 0),
      edad_prom_occ   = ifelse(n_occ > 0, mean(P6040[Oc == 1], na.rm = TRUE), 0),

      # ===== SEGUNDO TRABAJO ===================================================
      segundo_trabajo_hogar   = as.integer(any(P7040 == 1, na.rm = TRUE)),
      horas_segundo_trabajo_prom = ifelse(any(P7040 == 1, na.rm = TRUE),
                                          mean(P7045[P7040 == 1], na.rm = TRUE), 0),

      # ===== DESOCUPACIÓN CON INGRESOS (CLASIFICACIÓN) ========================
      n_desoc_con_ingresos   = sum(Des == 1 & (P7422 == 1 | P7472 == 1), na.rm = TRUE),
      
      desoc_con_ingresos_cat_chr = dplyr::case_when(
        n_des == 0 ~ "sin_desocupados",
        n_des > 0 & n_desoc_con_ingresos > 0 ~ "desoc_con_ingresos",
        n_des > 0 & n_desoc_con_ingresos == 0 ~ "desoc_sin_ingresos",
        TRUE ~ "sin_desocupados"
      ),

      # ===== TRABAJO INFANTIL / MAYORES (AGREGADO HOGAR) ======================
      has_child_work_no_study = any(Clase == 1 & P6040 >= 10 & P6040 <= 14 &
                                    Oc == 1 & P6240 != 3, na.rm = TRUE),
      has_child_work_study    = any(Clase == 1 & P6040 >= 10 & P6040 <= 14 &
                                    Oc == 1 & P6240 == 3, na.rm = TRUE),
      child_work_cat_chr = dplyr::case_when(
        has_child_work_no_study ~ "work_no_study",
        has_child_work_study    ~ "work_study",
        TRUE ~ "none"
      ),
      has_senior_calificado   = any(P6040 >= 65 & Oc == 1 & edu_years >= 16, na.rm = TRUE),
      has_senior_no_cal       = any(P6040 >= 65 & Oc == 1 & edu_years < 16, na.rm = TRUE),
      senior_work_cat_chr = dplyr::case_when(
        has_senior_calificado ~ "calificado",
        has_senior_no_cal     ~ "no_calificado",
        TRUE ~ "none"
      ),

      # ===== INGRESOS NO LABORALES (INDICADORES) ===============================
      ingreso_por_activos = as.integer(any(P7495 == 1 | P7500s2 == 1 | P7510s5 == 1, na.rm = TRUE)),
      ingreso_otros       = as.integer(any(P7505 == 1 | P7510s7 == 1, na.rm = TRUE)),
      ingreso_ayudas      = as.integer(any(P7510s1 == 1 | P7510s2 == 1 | P7510s3 == 1 |
                                           P7500s3 == 1 | P7510s7 == 1, na.rm = TRUE)),

      .groups = "drop"
    ) %>%
    mutate(
      # ===== FACTORES CON NIVELES EXPLÍCITOS ==================================
      desoc_con_ingresos_cat = factor(desoc_con_ingresos_cat_chr,
                                      levels = c("sin_desocupados", "desoc_con_ingresos", "desoc_sin_ingresos")),
      child_work_cat  = factor(child_work_cat_chr,
                               levels = c("none", "work_study", "work_no_study")),
      senior_work_cat = factor(senior_work_cat_chr,
                               levels = c("none", "calificado", "no_calificado"))
    ) %>%
    # ===== ORDEN FINAL DE COLUMNAS ============================================
    dplyr::select(
      # ID
      id,
      # Conteos
      n_personas, n_occ, n_des, n_ina, n_pet,
      n_pet_flex, n_no_pet_flex, n_occ_in_pet_flex,
      # Composición
      prop_occ_nper, prop_des_nper, prop_ina_nper,
      prop_no_pet_nper, prop_occ_pet, prop_occ_pet_flex,
      # Jefe del hogar
      edad_jefe, sexo_jefe, tam_emp_jefe, cotiza_pens_jefe, pos_ocup_jefe,
      antig_jefe, jefe_max_edu, jefe_max_edu_years, jefe_ocupado,
      prestaciones_jefe, horas_jefe,
      # Calidad del empleo
      prestaciones, prop_prest_occ,
      # Ocupados: horas, tamaño, antigüedad, educación y edad
      prom_horas_occ, tam_emp_prom_occ, antig_prom_occ, mean_edu_occ, edad_prom_occ,
      # Segundo trabajo
      segundo_trabajo_hogar, horas_segundo_trabajo_prom,
      # Desocupación
      n_desoc_con_ingresos, desoc_con_ingresos_cat,
      # Trabajo infantil y mayores
      child_work_cat, senior_work_cat,
      # Ingresos no laborales
      ingreso_por_activos, ingreso_otros, ingreso_ayudas
    )
}

```

```{r}
# Aplicar al TRAIN: personas -> hogares
train_agg <- agg_personas_to_hogares_plus_2(df_train_personas)
train_hogares_enriquecido <- df_train_hogares %>% left_join(train_agg, by = "id")

# Aplicar al TEST: personas -> hogares
test_agg <- agg_personas_to_hogares_plus_2(df_test_personas)
test_hogares_enriquecido  <- df_test_hogares  %>% left_join(test_agg,  by = "id")
```

```{r}
# ============================================================
# Variables base (las nuevas + básicas de hogar)
# ============================================================

vars <- c(
  # ---- Variables base del hogar ----
  "P5000","P5010","P5090","P5100","P5130","P5140",
  "Nper","Npersug","Depto","Lp",
  
  # ---- Conteos ----
  "n_personas","n_occ","n_des","n_ina","n_pet",
  "n_pet_flex","n_no_pet_flex","n_occ_in_pet_flex",
  
  # ---- Composición ----
  "prop_occ_nper","prop_des_nper","prop_ina_nper",
  "prop_no_pet_nper","prop_occ_pet","prop_occ_pet_flex",
  
  # ---- Jefe del hogar ----
  "edad_jefe","sexo_jefe","tam_emp_jefe","cotiza_pens_jefe",
  "pos_ocup_jefe","antig_jefe","jefe_max_edu",
  "jefe_max_edu_years","jefe_ocupado",
  "prestaciones_jefe","horas_jefe",
  
  # ---- Calidad del empleo ----
  "prestaciones","prop_prest_occ",
  
  # ---- Ocupados (promedios del hogar) ----
  "prom_horas_occ","tam_emp_prom_occ","antig_prom_occ",
  "mean_edu_occ","edad_prom_occ",
  
  # ---- Segundo trabajo ----
  "segundo_trabajo_hogar","horas_segundo_trabajo_prom",
  
  # ---- Desocupación ----
  "n_desoc_con_ingresos","desoc_con_ingresos_cat",
  
  # ---- Trabajo infantil y mayores ----
  "child_work_cat","senior_work_cat",
  
  # ---- Ingresos no laborales ----
  "ingreso_por_activos","ingreso_otros","ingreso_ayudas")

# ============================================================
# Limpieza de duplicados después de los joins
# ============================================================

train_hogares_enriquecido <- train_hogares_enriquecido %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  distinct(id, .keep_all = TRUE)

test_hogares_enriquecido <- test_hogares_enriquecido %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  distinct(id, .keep_all = TRUE)

# ============================================================
# Creación de interacciones y transformaciones
# ============================================================

train_hogares_enriquecido <- train_hogares_enriquecido %>%
  mutate(
    # ---- Potencias (efectos no lineales) ----
    edad_jefe2       = edad_jefe^2,
    edad_prom_occ2   = edad_prom_occ^2,

    # ---- Interacciones originales ----
    sexo_desocup     = sexo_jefe * prop_des_nper,
    edu_occ_lp       = mean_edu_occ * Lp,
    horasxedu_occ    = prom_horas_occ * mean_edu_occ,
    horasxpropocc    = prom_horas_occ * prop_occ_nper,

    # ---- Nuevas interacciones principales ----
    meanedu_x_propocc = mean_edu_occ * prop_occ_nper,
    propocc_x_lp      = prop_occ_nper * Lp,
    propocc_x_prest   = prop_occ_nper * prop_prest_occ
  )

# ---- Replicar mutaciones para test ----
test_hogares_enriquecido <- test_hogares_enriquecido %>%
  mutate(
    edad_jefe2       = edad_jefe^2,
    edad_prom_occ2   = edad_prom_occ^2,
    sexo_desocup     = sexo_jefe * prop_des_nper,
    edu_occ_lp       = mean_edu_occ * Lp,
    horasxedu_occ    = prom_horas_occ * mean_edu_occ,
    horasxpropocc    = prom_horas_occ * prop_occ_nper,
    meanedu_x_propocc = mean_edu_occ * prop_occ_nper,
    propocc_x_lp      = prop_occ_nper * Lp,
    propocc_x_prest   = prop_occ_nper * prop_prest_occ
  )

# ============================================================
# Añadir nuevas al vector de variables
# ============================================================

vars <- c(
  vars,
  "edad_jefe2","edad_prom_occ2","sexo_desocup",
  "edu_occ_lp","horasxedu_occ", "horasxpropocc",
  "meanedu_x_propocc","propocc_x_lp","propocc_x_prest"
)

# ============================================================
# Armar train/test finales para modelado
# ============================================================

train_use <- train_hogares_enriquecido %>%
  select(Pobre, all_of(vars)) %>%
  mutate(Pobre = factor(Pobre, levels = c(0,1), labels = c("no_pobre","pobre")))

test_final <- test_hogares_enriquecido %>%
  select(all_of(vars))

# ============================================================
# Definir categóricas y numéricas
# ============================================================

# Categóricas
cat_vars <- c(
  "Depto","P5090",
  "sexo_jefe","pos_ocup_jefe","cotiza_pens_jefe","jefe_max_edu",
  "child_work_cat","senior_work_cat","desoc_con_ingresos_cat"
)

# Definir numéricas como todo lo demás
num_vars <- setdiff(vars, cat_vars)

# ---- Convertir tipo adecuadamente ----
train_use[cat_vars]  <- lapply(train_use[cat_vars],  factor)
test_final[cat_vars] <- lapply(test_final[cat_vars], as.character)

# ---- Forzar conversión a numérico (maneja factors o caracteres)
for (v in c("P5100","P5130","P5140")) {
  train_use[[v]] <- as.numeric(as.character(train_use[[v]]))
  test_final[[v]] <- as.numeric(as.character(test_final[[v]]))
}

# ============================================================
# IMPUTACIÓN
# ============================================================

# 0 para numericas faltantes
for (v in num_vars) {
  train_use[[v]][is.na(train_use[[v]])] <- 0
  test_final[[v]][is.na(test_final[[v]])] <- 0
}

# cotiza_pens_jefe: imputar NA a "2" (No)
train_use$cotiza_pens_jefe[is.na(train_use$cotiza_pens_jefe)] <- "2"
test_final$cotiza_pens_jefe[is.na(test_final$cotiza_pens_jefe)] <- "2"

# ppos_ocup_jefe: crear nuevo nivel "0" y asignar a NA
levels(train_use$pos_ocup_jefe) <- c(levels(train_use$pos_ocup_jefe), "0")
levels(test_final$pos_ocup_jefe) <- c(levels(test_final$pos_ocup_jefe), "0")

train_use$pos_ocup_jefe[is.na(train_use$pos_ocup_jefe)] <- "0"
test_final$pos_ocup_jefe[is.na(test_final$pos_ocup_jefe)] <- "0"

for (v in cat_vars) {
  niveles <- union(levels(train_use[[v]]), unique(test_final[[v]]))
  train_use[[v]] <- factor(train_use[[v]], levels = niveles)
  test_final[[v]] <- factor(test_final[[v]], levels = niveles)
}
# ============= Preparar Paralelización ===============

# Detectar el número de núcleos disponibles
n_cores <- detectCores() - 4  # deja 1 libre para el sistema

# Crear el clúster
cl <- makeCluster(n_cores)

# Registrar el clúster para que caret lo use
registerDoParallel(cl)

# Confirmar
getDoParWorkers()

#==========================

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)

set.seed(2050)  # Fijamos semilla para reproducibilidad

p_load("ranger")
cat("Entrenando modelo con", getDoParWorkers(), "núcleos...\n")
start_time <- Sys.time()
tree_ranger_grid <-train (
    Pobre ~ .,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl, # Especificamos los controles de entrenamiento
    verbose=TRUE,#Mirar progreso en el panel de control
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 200,
    importance = "permutation" 
)
end_time <- Sys.time()
cat("Tiempo total de entrenamiento:", round(difftime(end_time, start_time, units = "mins"), 2), "minutos\n")
stopCluster(cl)
registerDoSEQ()

tree_ranger_grid

var_imp=varImp(tree_ranger_grid)
df_vi=data.frame(var_imp$importance)
df_vi$importance <- as.integer(df_vi$Overall)
df_vi$feature <- rownames(df_vi)
df_vi <- df_vi[order(-df_vi$Overall), ]

write_xlsx(VI, here::here("stores", "VI_exportado.xlsx"))

p_load("pROC") # Paquete para calcular y visualizar curvas ROC

roc_obj_en<-roc(response=tree_ranger_grid$pred$obs,  # Valores reales de la variable objetivo
                   predictor=tree_ranger_grid$pred$pobre, # Probabilidades predichas por el modelo
                  levels = c("no_pobre", "pobre"),  # # Establece la referencia control y caso (empleado = negativo, desempleado = positivo) 
                  direction = "<")  # "<" significa que "desempleado" es positivo
     
prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))
prec_recall

prec_recall<- prec_recall  %>% mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

# Predicción de probabilidad de desempleo
test_RFPR<- test_final %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "prob")) 

# Clasificación según el umbral óptimo de F1
test_RFPR <- test_RFPR %>%
  mutate(pobre_hat_rangerPRs_cutoff_F1 = factor(
    ifelse(pobre_hat_ranger$pobre >= prec_recall$threshold[which.max(prec_recall$F1)], 1, 0),
    levels = c(1, 0),
    labels = c("pobre", "no_pobre")
  ))
probRF_1 <- ifelse (test_RFPR$pobre_hat_rangerPRs_cutoff_F1=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- prec_recall$threshold[which.max(prec_recall$F1)]

nombre_archivoRF <- sprintf(
  "RF_mtry_%s_min.node.size_%s_SR_%s_update.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))

```


```{r}

# Asegúrate de cargar dplyr
library(dplyr)

# Filtrar df_vi
df_vi <- df_vi[df_vi$importance > 9, ]

# Crear vector de variables importantes
vars_imp <- df_vi$feature

# Para train_use
vars_imp_existentes <- vars_imp[vars_imp %in% names(train_use)]
# Para test_final
vars_imp_existentes_test <- vars_imp[vars_imp %in% names(test_final)]

# Seleccionar columnas en train_use
train_use <- train_use %>%
  dplyr::select(Pobre, all_of(vars_imp_existentes))

# Seleccionar columnas en test_final
test_final <- test_final %>%
  dplyr::select(all_of(vars_imp_existentes_test))

# Para train_use
vars_imp_existentes <- vars_imp[vars_imp %in% names(train_use)]
# Para test_final
vars_imp_existentes_test <- vars_imp[vars_imp %in% names(test_final)]


# ============= Preparar Paralelización ===============

# Detectar el número de núcleos disponibles
n_cores <- detectCores() - 4  # deja 1 libre para el sistema

# Crear el clúster
cl <- makeCluster(n_cores)

# Registrar el clúster para que caret lo use
registerDoParallel(cl)

# Confirmar
getDoParWorkers()

#==========================



ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)

set.seed(2050)  # Fijamos semilla para reproducibilidad


p_load("ranger")
cat("Entrenando modelo con", getDoParWorkers(), "núcleos...\n")
start_time <- Sys.time()
tree_ranger_grid <-train (
    Pobre ~ .,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl, # Especificamos los controles de entrenamiento
    verbose=TRUE,#Mirar progreso en el panel de control
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 200 
)

end_time <- Sys.time()
cat("Tiempo total de entrenamiento:", round(difftime(end_time, start_time, units = "mins"), 2), "minutos\n")
stopCluster(cl)
registerDoSEQ()

tree_ranger_grid

p_load("pROC") # Paquete para calcular y visualizar curvas ROC

roc_obj_en<-roc(response=tree_ranger_grid$pred$obs,  # Valores reales de la variable objetivo
                   predictor=tree_ranger_grid$pred$pobre, # Probabilidades predichas por el modelo
                  levels = c("no_pobre", "pobre"),  # # Establece la referencia control y caso (empleado = negativo, desempleado = positivo) 
                  direction = "<")  # "<" significa que "desempleado" es positivo
     
prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))
prec_recall

prec_recall<- prec_recall  %>% mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

# Predicción de probabilidad de desempleo
test_RFPR<- test_final %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "prob")) 

# Clasificación según el umbral óptimo de F1
test_RFPR <- test_RFPR %>%
  mutate(pobre_hat_rangerPRs_cutoff_F1 = factor(
    ifelse(pobre_hat_ranger$pobre >= prec_recall$threshold[which.max(prec_recall$F1)], 1, 0),
    levels = c(1, 0),
    labels = c("pobre", "no_pobre")
  ))
probRF_1 <- ifelse (test_RFPR$pobre_hat_rangerPRs_cutoff_F1=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- prec_recall$threshold[which.max(prec_recall$F1)]

nombre_archivoRF <- sprintf(
  "VI_RF_mtry_%s_min.node.size_%s_SR_%s_update.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))

```
