---
title: "Trees"
author: "G4"
date: "2025-10-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(pacman)

p_load(rio, # Import/export data.
       tidyverse, # Tidy-data.
       stargazer, # Descriptive statistics.
       gt, # Descriptive statistics.
       gtsummary,
       caret, # For predictive model assessment.
       gridExtra, # Arrange plots.
       skimr, # Summarize data.
       here, #for file searching
       readr, #for opening files
       ggplot2,
       boot,
       scales,
       car,
       dplyr,
       tidyr,
       plurrr,
       MLmetrics, 
       glmnet,
       paralell,
       doParallel
       )
```

```{r}
########DATA MODEL 2

ruta7 <- here("stores", "train_hogares_enriquecido_2.csv")
train_hogares_enriquecido <- read_csv(ruta7)

ruta8 <- here("stores", "test_hogares_enriquecido_2.csv")
test_hogares_enriquecido <- read_csv(ruta8)

fiveStats <- function(...) c(prSummary(...))

```

```{r}

# Variables base (las nuevas + básicas de hogar)
vars <- c(
  "P5000","P5010","P5090","P5100","P5130","P5140",
  "Nper","Npersug","Depto","Lp",
  # variables de personas a nivel hogar
  "mean_edu_occ","prop_occ_nper","prop_occ_pet","jefe_max_edu","horas_prom_occ",
  "sexo_jefe","antig_jefe","antig_prom_occ","edu_prom_occ","prop_des",
  "edad_jefe","edad_prom_occ","prestaciones","tam_emp_jefe","tam_emp_prom_occ",
  "cotiza_pens_jefe","pos_ocup_jefe"
)

# Limpia duplicados después de hacer todos los joins
train_hogares_enriquecido <- train_hogares_enriquecido %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  distinct(id, .keep_all = TRUE)

test_hogares_enriquecido <- test_hogares_enriquecido %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  distinct(id, .keep_all = TRUE)

# Mutar
train_hogares_enriquecido <- train_hogares_enriquecido %>%
  mutate(
    edad_jefe2       = edad_jefe^2,
    edad_prom_occ2   = edad_prom_occ^2,
    sexo_desocup     = sexo_jefe * prop_des,
    edu_occ_lp       = mean_edu_occ * Lp,
    horasxedu_occ    = horas_prom_occ * mean_edu_occ   # ← aquí el cambio
  )

test_hogares_enriquecido <- test_hogares_enriquecido %>%
  mutate(
    edad_jefe2       = edad_jefe^2,
    edad_prom_occ2   = edad_prom_occ^2,
    sexo_desocup     = sexo_jefe * prop_des,
    edu_occ_lp       = mean_edu_occ * Lp,
    horasxedu_occ    = horas_prom_occ * mean_edu_occ
  )


# Añadir nuevas al vector de variables
vars <- c(vars, "edad_jefe2", "edad_prom_occ2", "sexo_desocup", "edu_occ_lp", "horasxedu_occ")

# ============================================================
# Armar train/test
# ============================================================

train_use <- train_hogares_enriquecido %>%
  select(Pobre, all_of(vars)) %>%
  mutate(Pobre = factor(Pobre, levels = c(0,1), labels = c("no_pobre","pobre")))

test_final <- test_hogares_enriquecido %>%
  select(all_of(vars))

# Categóricas (igual que antes)
cat_vars <- c("Depto","P5090","pos_ocup_jefe","cotiza_pens_jefe")
num_vars <- setdiff(vars, cat_vars)

train_use[cat_vars]  <- lapply(train_use[cat_vars],  factor)
test_final[cat_vars] <- lapply(test_final[cat_vars], as.character)

# ============================================================
# IMPUTACIÓN (Considerando que este código tiene probemas de imputación identificados)
# ============================================================
#
# Numéricas -> mediana
for (v in num_vars) {
  med <- median(train_use[[v]], na.rm = TRUE)
  train_use[[v]][is.na(train_use[[v]])] <- med
  test_final[[v]][is.na(test_final[[v]])] <- med
}

# Categóricas -> moda
for (v in cat_vars) {
  mode_v <- names(which.max(table(train_use[[v]])))
  train_use[[v]][is.na(train_use[[v]])] <- mode_v
  tmp <- test_final[[v]]
  tmp[is.na(tmp) | !(tmp %in% levels(train_use[[v]]))] <- mode_v
  test_final[[v]] <- factor(tmp, levels = levels(train_use[[v]]))
}
train_use <- droplevels(train_use)

```

```{r}
#########################################################
####### CART #1 (Modelo con variables iniciales) ########
#########################################################

#F1 = 0.48
form <- as.formula(paste("Pobre ~", paste(common_preds, collapse = " + ")))

ctrl_F1 <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = prSummary,  # calcula AUC-PR, F, Precision, Recall
  savePredictions = "final"
)

grid_dt <- expand.grid(maxdepth = 2:10)

set.seed(123)
dt_clasif_F1 <- train(
  form,
  data       = train_imp %>% select(-id),
  method     = "rpart2",
  trControl  = ctrl_F1,
  tuneGrid   = grid_dt,
  metric     = "F"
)

print(dt_clasif_F1)
print(dt_clasif_F1$bestTune)

test_pred_cls <- predict(dt_clasif_F1, newdata = test_imp, type = "raw")  # "yes"/"no"

kaggle_sub <- test_imp %>%
  transmute(
    id    = as.character(id),
    Pobre = as.integer(test_pred_cls == "yes")
  )

write_csv(kaggle_sub, here::here("stores", "CART_clasif_F1.csv"))

cat("Archivo guardado en:", here::here("stores", "CART_clasif_F1.csv"), "\n")


```

```{r}
##################################################################
####### RANDOM FOREST #1 (Modelo con variables iniciales) ########
##################################################################

#F1 = 0.65

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)


set.seed(2050)  # Fijamos semilla para reproducibilidad

p_load("ranger")
tree_ranger_grid <-train (
    Pobre ~ . ,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl,  # Especificamos los controles de entrenamiento
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 200
)
tree_ranger_grid


test_RF<- test_final  %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "raw"))
probRF_1 <- ifelse (test_RF$pobre_hat_ranger=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- tree_ranger_grid$bestTune$splitrule

nombre_archivoRF <- sprintf(
  "RF_mtry_%s_min.node.size_%s_SR_%s.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))
```

```{r}
####################################################################################
####### RANDOM FOREST #2 (Optimizamos threshold con Precision recall Curve) ########
####################################################################################

#F1 = 0.68

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)


set.seed(2050)  # Fijamos semilla para reproducibilidad

p_load("ranger")
tree_ranger_grid <-train (
    Pobre ~ . ,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl,  # Especificamos los controles de entrenamiento
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 200
)
tree_ranger_grid

p_load("pROC") # Paquete para calcular y visualizar curvas ROC

roc_obj_en<-roc(response=tree_ranger_grid$pred$obs,  # Valores reales de la variable objetivo
                   predictor=tree_ranger_grid$pred$pobre, # Probabilidades predichas por el modelo
                  levels = c("no_pobre", "pobre"),  # # Establece la referencia control y caso (empleado = negativo, desempleado = positivo) 
                  direction = "<")  # "<" significa que "desempleado" es positivo
     
prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))
prec_recall

prec_recall<- prec_recall  %>% mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

# Predicción de probabilidad de desempleo
test_RFPR<- test_final  %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "prob"))

# Clasificación según el umbral óptimo de F1
test_RFPR <- test_RFPR %>%
  mutate(pobre_hat_rangerPRs_cutoff_F1 = factor(
    ifelse(pobre_hat_ranger$pobre >= prec_recall$threshold[which.max(prec_recall$F1)], 1, 0),
    levels = c(1, 0),
    labels = c("pobre", "no_pobre")
  ))
probRF_1 <- ifelse (test_RFPR$pobre_hat_rangerPRs_cutoff_F1=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- prec_recall$threshold[which.max(prec_recall$F1)]

nombre_archivoRF <- sprintf(
  "RF_mtry_%s_min.node.size_%s_SR_%s.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))

```


```{r}

#################################################################################################
####### RANDOM FOREST #3 (Optimizamos threshold con Precision recall Curve + imputación) ########
#################################################################################################

#F1 = 0.69

##Nueva imputación
# Numéricas_0 -> mediana
for (v in num_vars) {
  train_use[[v]][is.na(train_use[[v]])] <- 0
  test_final[[v]][is.na(test_final[[v]])] <- 0
}

# cotiza_pens_jefe: imputar NA a "2" (No)
train_use$cotiza_pens_jefe[is.na(train_use$cotiza_pens_jefe)] <- "2"
test_final$cotiza_pens_jefe[is.na(test_final$cotiza_pens_jefe)] <- "2"

# ppos_ocup_jefe: crear nuevo nivel "0" y asignar a NA
levels(train_use$pos_ocup_jefe) <- c(levels(train_use$pos_ocup_jefe), "0")
levels(test_final$pos_ocup_jefe) <- c(levels(test_final$pos_ocup_jefe), "0")

train_use$pos_ocup_jefe[is.na(train_use$pos_ocup_jefe)] <- "0"
test_final$pos_ocup_jefe[is.na(test_final$pos_ocup_jefe)] <- "0"

train_use <- droplevels(train_use)

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)

set.seed(2050)  # Fijamos semilla para reproducibilidad

p_load("ranger")
tree_ranger_grid <-train (
    Pobre ~ . ,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl,  # Especificamos los controles de entrenamiento
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 200
)
tree_ranger_grid


hola=varImp(tree_ranger_grid)

p_load("pROC") # Paquete para calcular y visualizar curvas ROC

roc_obj_en<-roc(response=tree_ranger_grid$pred$obs,  # Valores reales de la variable objetivo
                   predictor=tree_ranger_grid$pred$pobre, # Probabilidades predichas por el modelo
                  levels = c("no_pobre", "pobre"),  # # Establece la referencia control y caso (empleado = negativo, desempleado = positivo) 
                  direction = "<")  # "<" significa que "desempleado" es positivo
     
prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))
prec_recall

prec_recall<- prec_recall  %>% mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

# Predicción de probabilidad de desempleo
test_RFPR<- test_final  %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "prob"))

# Clasificación según el umbral óptimo de F1
test_RFPR <- test_RFPR %>%
  mutate(pobre_hat_rangerPRs_cutoff_F1 = factor(
    ifelse(pobre_hat_ranger$pobre >= prec_recall$threshold[which.max(prec_recall$F1)], 1, 0),
    levels = c(1, 0),
    labels = c("pobre", "no_pobre")
  ))
probRF_1 <- ifelse (test_RFPR$pobre_hat_rangerPRs_cutoff_F1=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- prec_recall$threshold[which.max(prec_recall$F1)]

nombre_archivoRF <- sprintf(
  "RF_mtry_%s_min.node.size_%s_SR_%s2.0.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))

```

```{r}

#################################################################################################
####### RANDOM FOREST #4 (Optimizamos threshold con Precision recall Curve + imputación + nuevas variables) ########
#################################################################################################

#F1 = 0.71

ruta1 <- here("stores", "train_hogares.csv")
df_train_hogares <- read_csv(ruta1)

ruta2 <- here("stores", "train_personas.csv")
df_train_personas <- read_csv(ruta2)

ruta3 <- here("stores", "test_personas.csv")
df_test_personas <- read_csv(ruta3)

ruta4 <- here("stores", "test_hogares.csv")
df_test_hogares <- read_csv(ruta4)

# Create new variables at individual level
# Function to create years of education
add_edu_years <- function(df) {
  df$edu_years <- ifelse(
    df$P6210 %in% c(1, 2, 9), 0, # None, Preschool, Unknown → 0
    ifelse(
      df$P6210 %in% c(3, 4, 5), df$P6210s1, # Primary or Secondary → as is
      ifelse(df$P6210 == 6, df$P6210s1 + 11, NA_real_) # Tertiary → add 11
    )
  )
  df
}

# Function to create years of experience (Mincer-style proxy)
# exp = age - edu_years - 6
add_experience <- function(df) {
  df$exp <- df$P6040 - df$edu_years - 6
  df
}

# Function to Manage NA's on labor indicators
fill_labor_nas <- function(df) {
  vars <- c("Pet", "Oc", "Des", "Ina")
  df %>%
    mutate(across(all_of(vars), ~ as.integer(replace(., is.na(.), 0L))))
}

# Define PET_flexible (Urbano: 15–64 años; Rural: 12–69 años)
add_pet_flexible <- function(df) {
  df$PET_flexible <- ifelse(
    is.na(df$P6040), NA_integer_,
    ifelse(
      df$Clase == 1,                           # Urbano
      ifelse(df$P6040 >= 15 & df$P6040 <= 64, 1L, 0L),
      ifelse(df$P6040 >= 12 & df$P6040 <= 69, 1L, 0L) # Rural
    )
  )
  df
}

# Apply functiona to Train and Test
df_train_personas <- add_edu_years(df_train_personas)
df_test_personas  <- add_edu_years(df_test_personas)

df_train_personas <- add_experience(df_train_personas)
df_test_personas  <- add_experience(df_test_personas)

df_train_personas <- fill_labor_nas(df_train_personas)
df_test_personas  <- fill_labor_nas(df_test_personas)

df_train_personas <- add_pet_flexible(df_train_personas)
df_test_personas  <- add_pet_flexible(df_test_personas)

# Agreggate variables to household level
agg_personas_to_hogares_plus_2 <- function(df_personas) {

  # Helpers para robustez numérica
  safe_div <- function(a, b) ifelse(is.na(b) | b <= 0, 0, a / b)

  df_personas %>%
    group_by(id) %>%
    summarise(
      # ===== BASE DE CONTEOS ===================================================
      n_personas = n(),
      n_occ      = sum(Oc  == 1, na.rm = TRUE),
      n_des      = sum(Des == 1, na.rm = TRUE),
      n_ina      = sum(Ina == 1, na.rm = TRUE),
      n_pet      = sum(Pet == 1, na.rm = TRUE),

      # ===== PET flexible ===
      n_pet_flex        = sum(PET_flexible == 1, na.rm = TRUE),
      n_no_pet_flex     = n_personas - n_pet_flex,
      n_occ_in_pet_flex = sum(Oc == 1 & PET_flexible == 1, na.rm = TRUE),
      

      # ===== COMPOSICIÓN / PROPORCIONES (LEGADO + NUEVAS) =====================
      prop_occ_nper     = safe_div(n_occ, n_personas),
      prop_des_nper     = safe_div(n_des, n_personas),         # NUEVA
      prop_ina_nper     = safe_div(n_ina, n_personas),         # NUEVA
      prop_no_pet_nper  = safe_div(n_no_pet_flex, n_personas), # NUEVA (PET flexible)
      prop_occ_pet      = safe_div(n_occ, n_pet),              # LEGADO (PET original)
      prop_occ_pet_flex = safe_div(n_occ_in_pet_flex, n_pet_flex), # NUEVA (PET flexible)

      # ===== JEFE DEL HOGAR: CARACTERÍSTICAS ==================================
      edad_jefe       = dplyr::first(P6040[P6050 == 1]),
      sexo_jefe       = dplyr::first(P6020[P6050 == 1]),
      tam_emp_jefe    = dplyr::coalesce(dplyr::first(P6870[P6050 == 1]), 0),
      cotiza_pens_jefe= dplyr::coalesce(dplyr::first(P6920[P6050 == 1]), 0),
      pos_ocup_jefe   = dplyr::coalesce(dplyr::first(P6430[P6050 == 1]), 0),
      antig_jefe      = dplyr::coalesce(dplyr::first(P6426[P6050 == 1]), 0), # si NA -> 0
      jefe_max_edu    = max(P6210[P6050 == 1], na.rm = TRUE),  # LEGADO (cód. educ. máx.)
      jefe_max_edu_years = {
        val <- max(P6210[P6050 == 1], na.rm = TRUE)
        if (is.finite(val)) val else 0
      },
      
      # Indicadores del jefe
      jefe_ocupado     = as.integer(any(P6050 == 1 & Oc == 1, na.rm = TRUE)),
      prestaciones_jefe= as.integer(any(P6050 == 1 &
                                  (P6510 == 1 | P6545 == 1 | P6580 == 1 |
                                   P6630s1 == 1 | P6630s2 == 1 | P6630s3 == 1 |
                                   P6630s4 == 1 | P6630s6 == 1),
                                  na.rm = TRUE)),
      horas_jefe       = ifelse(any(P6050 == 1 & Oc == 1, na.rm = TRUE),
                                dplyr::first(P6800[P6050 == 1 & Oc == 1]), 0),

      # ===== CALIDAD DEL EMPLEO (HOGAR) =======================================
      prestaciones = as.integer(
        any(P6510 == 1, na.rm = TRUE) |
        any(P6545 == 1, na.rm = TRUE) |
        any(P6580 == 1, na.rm = TRUE) |
        any(P6630s1 == 1, na.rm = TRUE) |
        any(P6630s2 == 1, na.rm = TRUE) |
        any(P6630s3 == 1, na.rm = TRUE) |
        any(P6630s4 == 1, na.rm = TRUE) |
        any(P6630s6 == 1, na.rm = TRUE)   # (prioriza versión LEGADA)
      ),
      
      prop_prest_occ = if (n_occ > 0) { 
        mean( 
          (P6510 == 1 | P6545 == 1 | P6580 == 1 | P6630s1 == 1 | P6630s2 == 1 | P6630s3 == 1 | P6630s4 == 1 | P6630s6 == 1)[Oc == 1], 
          na.rm = TRUE 
          ) 
        } else 0,

      # ===== HORAS Y TAMAÑOS (ENTRE OCUPADOS) =================================
      prom_horas_occ  = ifelse(n_occ > 0, mean(P6800[Oc == 1], na.rm = TRUE), 0), # alias LEGADO
      tam_emp_prom_occ= ifelse(n_occ > 0, mean(P6870[Oc == 1], na.rm = TRUE), 0),

      # ===== ANTIGÜEDAD / EDUCACIÓN / EDAD (ENTRE OCUPADOS) ===================
      antig_prom_occ  = ifelse(n_occ > 0, mean(P6426[Oc == 1], na.rm = TRUE), 0),
      mean_edu_occ    = ifelse(n_occ > 0, mean(edu_years[Oc == 1], na.rm = TRUE), 0),
      edad_prom_occ   = ifelse(n_occ > 0, mean(P6040[Oc == 1], na.rm = TRUE), 0),

      # ===== SEGUNDO TRABAJO ===================================================
      segundo_trabajo_hogar   = as.integer(any(P7040 == 1, na.rm = TRUE)),
      horas_segundo_trabajo_prom = ifelse(any(P7040 == 1, na.rm = TRUE),
                                          mean(P7045[P7040 == 1], na.rm = TRUE), 0),

      # ===== DESOCUPACIÓN CON INGRESOS (CLASIFICACIÓN) ========================
      n_desoc_con_ingresos   = sum(Des == 1 & (P7422 == 1 | P7472 == 1), na.rm = TRUE),
      
      desoc_con_ingresos_cat_chr = dplyr::case_when(
        n_des == 0 ~ "sin_desocupados",
        n_des > 0 & n_desoc_con_ingresos > 0 ~ "desoc_con_ingresos",
        n_des > 0 & n_desoc_con_ingresos == 0 ~ "desoc_sin_ingresos",
        TRUE ~ "sin_desocupados"
      ),

      # ===== TRABAJO INFANTIL / MAYORES (AGREGADO HOGAR) ======================
      has_child_work_no_study = any(Clase == 1 & P6040 >= 10 & P6040 <= 14 &
                                    Oc == 1 & P6240 != 3, na.rm = TRUE),
      has_child_work_study    = any(Clase == 1 & P6040 >= 10 & P6040 <= 14 &
                                    Oc == 1 & P6240 == 3, na.rm = TRUE),
      child_work_cat_chr = dplyr::case_when(
        has_child_work_no_study ~ "work_no_study",
        has_child_work_study    ~ "work_study",
        TRUE ~ "none"
      ),
      has_senior_calificado   = any(P6040 >= 65 & Oc == 1 & edu_years >= 16, na.rm = TRUE),
      has_senior_no_cal       = any(P6040 >= 65 & Oc == 1 & edu_years < 16, na.rm = TRUE),
      senior_work_cat_chr = dplyr::case_when(
        has_senior_calificado ~ "calificado",
        has_senior_no_cal     ~ "no_calificado",
        TRUE ~ "none"
      ),

      # ===== INGRESOS NO LABORALES (INDICADORES) ===============================
      ingreso_por_activos = as.integer(any(P7495 == 1 | P7500s2 == 1 | P7510s5 == 1, na.rm = TRUE)),
      ingreso_otros       = as.integer(any(P7505 == 1 | P7510s7 == 1, na.rm = TRUE)),
      ingreso_ayudas      = as.integer(any(P7510s1 == 1 | P7510s2 == 1 | P7510s3 == 1 |
                                           P7500s3 == 1 | P7510s7 == 1, na.rm = TRUE)),

      .groups = "drop"
    ) %>%
    mutate(
      # ===== FACTORES CON NIVELES EXPLÍCITOS ==================================
      desoc_con_ingresos_cat = factor(desoc_con_ingresos_cat_chr,
                                      levels = c("sin_desocupados", "desoc_con_ingresos", "desoc_sin_ingresos")),
      child_work_cat  = factor(child_work_cat_chr,
                               levels = c("none", "work_study", "work_no_study")),
      senior_work_cat = factor(senior_work_cat_chr,
                               levels = c("none", "calificado", "no_calificado"))
    ) %>%
    # ===== ORDEN FINAL DE COLUMNAS ============================================
    dplyr::select(
      # ID
      id,
      # Conteos
      n_personas, n_occ, n_des, n_ina, n_pet,
      n_pet_flex, n_no_pet_flex, n_occ_in_pet_flex,
      # Composición
      prop_occ_nper, prop_des_nper, prop_ina_nper,
      prop_no_pet_nper, prop_occ_pet, prop_occ_pet_flex,
      # Jefe del hogar
      edad_jefe, sexo_jefe, tam_emp_jefe, cotiza_pens_jefe, pos_ocup_jefe,
      antig_jefe, jefe_max_edu, jefe_max_edu_years, jefe_ocupado,
      prestaciones_jefe, horas_jefe,
      # Calidad del empleo
      prestaciones, prop_prest_occ,
      # Ocupados: horas, tamaño, antigüedad, educación y edad
      prom_horas_occ, tam_emp_prom_occ, antig_prom_occ, mean_edu_occ, edad_prom_occ,
      # Segundo trabajo
      segundo_trabajo_hogar, horas_segundo_trabajo_prom,
      # Desocupación
      n_desoc_con_ingresos, desoc_con_ingresos_cat,
      # Trabajo infantil y mayores
      child_work_cat, senior_work_cat,
      # Ingresos no laborales
      ingreso_por_activos, ingreso_otros, ingreso_ayudas
    )
}

# Aplicar al TRAIN: personas -> hogares
train_agg <- agg_personas_to_hogares_plus_2(df_train_personas)
train_hogares_enriquecido <- df_train_hogares %>% left_join(train_agg, by = "id")

# Aplicar al TEST: personas -> hogares
test_agg <- agg_personas_to_hogares_plus_2(df_test_personas)
test_hogares_enriquecido  <- df_test_hogares  %>% left_join(test_agg,  by = "id")

# ============================================================
# Variables base (las nuevas + básicas de hogar)
# ============================================================

vars <- c(
  # ---- Variables base del hogar ----
  "P5000","P5010","P5090","P5100","P5130","P5140",
  "Nper","Npersug","Depto","Lp",
  
  # ---- Conteos ----
  "n_personas","n_occ","n_des","n_ina","n_pet",
  "n_pet_flex","n_no_pet_flex","n_occ_in_pet_flex",
  
  # ---- Composición ----
  "prop_occ_nper","prop_des_nper","prop_ina_nper",
  "prop_no_pet_nper","prop_occ_pet","prop_occ_pet_flex",
  
  # ---- Jefe del hogar ----
  "edad_jefe","sexo_jefe","tam_emp_jefe","cotiza_pens_jefe",
  "pos_ocup_jefe","antig_jefe","jefe_max_edu",
  "jefe_max_edu_years","jefe_ocupado",
  "prestaciones_jefe","horas_jefe",
  
  # ---- Calidad del empleo ----
  "prestaciones","prop_prest_occ",
  
  # ---- Ocupados (promedios del hogar) ----
  "prom_horas_occ","tam_emp_prom_occ","antig_prom_occ",
  "mean_edu_occ","edad_prom_occ",
  
  # ---- Segundo trabajo ----
  "segundo_trabajo_hogar","horas_segundo_trabajo_prom",
  
  # ---- Desocupación ----
  "n_desoc_con_ingresos","desoc_con_ingresos_cat",
  
  # ---- Trabajo infantil y mayores ----
  "child_work_cat","senior_work_cat",
  
  # ---- Ingresos no laborales ----
  "ingreso_por_activos","ingreso_otros","ingreso_ayudas")

# ============================================================
# Limpieza de duplicados después de los joins
# ============================================================

train_hogares_enriquecido <- train_hogares_enriquecido %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  distinct(id, .keep_all = TRUE)

test_hogares_enriquecido <- test_hogares_enriquecido %>%
  select(-ends_with(".x"), -ends_with(".y")) %>%
  distinct(id, .keep_all = TRUE)

# ============================================================
# Creación de interacciones y transformaciones
# ============================================================

train_hogares_enriquecido <- train_hogares_enriquecido %>%
  mutate(
    # ---- Potencias (efectos no lineales) ----
    edad_jefe2       = edad_jefe^2,
    edad_prom_occ2   = edad_prom_occ^2,

    # ---- Interacciones originales ----
    sexo_desocup     = sexo_jefe * prop_des_nper,
    edu_occ_lp       = mean_edu_occ * Lp,
    horasxedu_occ    = prom_horas_occ * mean_edu_occ,
    horasxpropocc    = prom_horas_occ * prop_occ_nper,

    # ---- Nuevas interacciones principales ----
    meanedu_x_propocc = mean_edu_occ * prop_occ_nper,
    propocc_x_lp      = prop_occ_nper * Lp,
    propocc_x_prest   = prop_occ_nper * prop_prest_occ
  )

# ---- Replicar mutaciones para test ----
test_hogares_enriquecido <- test_hogares_enriquecido %>%
  mutate(
    edad_jefe2       = edad_jefe^2,
    edad_prom_occ2   = edad_prom_occ^2,
    sexo_desocup     = sexo_jefe * prop_des_nper,
    edu_occ_lp       = mean_edu_occ * Lp,
    horasxedu_occ    = prom_horas_occ * mean_edu_occ,
    horasxpropocc    = prom_horas_occ * prop_occ_nper,
    meanedu_x_propocc = mean_edu_occ * prop_occ_nper,
    propocc_x_lp      = prop_occ_nper * Lp,
    propocc_x_prest   = prop_occ_nper * prop_prest_occ
  )

# ============================================================
# Añadir nuevas al vector de variables
# ============================================================

vars <- c(
  vars,
  "edad_jefe2","edad_prom_occ2","sexo_desocup",
  "edu_occ_lp","horasxedu_occ", "horasxpropocc",
  "meanedu_x_propocc","propocc_x_lp","propocc_x_prest"
)

# ============================================================
# Armar train/test finales para modelado
# ============================================================

train_use <- train_hogares_enriquecido %>%
  select(Pobre, all_of(vars)) %>%
  mutate(Pobre = factor(Pobre, levels = c(0,1), labels = c("no_pobre","pobre")))

test_final <- test_hogares_enriquecido %>%
  select(all_of(vars))

# ============================================================
# Definir categóricas y numéricas
# ============================================================

# Categóricas
cat_vars <- c(
  "Depto","P5090",
  "sexo_jefe","pos_ocup_jefe","cotiza_pens_jefe","jefe_max_edu",
  "child_work_cat","senior_work_cat","desoc_con_ingresos_cat"
)

# Definir numéricas como todo lo demás
num_vars <- setdiff(vars, cat_vars)

# ---- Convertir tipo adecuadamente ----
train_use[cat_vars]  <- lapply(train_use[cat_vars],  factor)
test_final[cat_vars] <- lapply(test_final[cat_vars], as.character)

# ---- Forzar conversión a numérico (maneja factors o caracteres)
for (v in c("P5100","P5130","P5140")) {
  train_use[[v]] <- as.numeric(as.character(train_use[[v]]))
  test_final[[v]] <- as.numeric(as.character(test_final[[v]]))
}

# ============================================================
# IMPUTACIÓN
# ============================================================

# 0 para numericas faltantes
for (v in num_vars) {
  train_use[[v]][is.na(train_use[[v]])] <- 0
  test_final[[v]][is.na(test_final[[v]])] <- 0
}

# cotiza_pens_jefe: imputar NA a "2" (No)
train_use$cotiza_pens_jefe[is.na(train_use$cotiza_pens_jefe)] <- "2"
test_final$cotiza_pens_jefe[is.na(test_final$cotiza_pens_jefe)] <- "2"

# ppos_ocup_jefe: crear nuevo nivel "0" y asignar a NA
levels(train_use$pos_ocup_jefe) <- c(levels(train_use$pos_ocup_jefe), "0")
levels(test_final$pos_ocup_jefe) <- c(levels(test_final$pos_ocup_jefe), "0")

train_use$pos_ocup_jefe[is.na(train_use$pos_ocup_jefe)] <- "0"
test_final$pos_ocup_jefe[is.na(test_final$pos_ocup_jefe)] <- "0"

for (v in cat_vars) {
  niveles <- union(levels(train_use[[v]]), unique(test_final[[v]]))
  train_use[[v]] <- factor(train_use[[v]], levels = niveles)
  test_final[[v]] <- factor(test_final[[v]], levels = niveles)
}
# ============= Preparar Paralelización ===============

# Detectar el número de núcleos disponibles
n_cores <- detectCores() - 4  # deja 1 libre para el sistema

# Crear el clúster
cl <- makeCluster(n_cores)

# Registrar el clúster para que caret lo use
registerDoParallel(cl)

# Confirmar
getDoParWorkers()

#======================================================

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)

set.seed(2050)  # Fijamos semilla para reproducibilidad

p_load("ranger")
cat("Entrenando modelo con", getDoParWorkers(), "núcleos...\n")
start_time <- Sys.time()
tree_ranger_grid <-train (
    Pobre ~ .,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl, # Especificamos los controles de entrenamiento
    verbose=TRUE,#Mirar progreso en el panel de control
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 200,
    importance = "permutation" 
)
end_time <- Sys.time()
cat("Tiempo total de entrenamiento:", round(difftime(end_time, start_time, units = "mins"), 2), "minutos\n")
stopCluster(cl)
registerDoSEQ()

tree_ranger_grid

p_load("pROC") # Paquete para calcular y visualizar curvas ROC

roc_obj_en<-roc(response=tree_ranger_grid$pred$obs,  # Valores reales de la variable objetivo
                   predictor=tree_ranger_grid$pred$pobre, # Probabilidades predichas por el modelo
                  levels = c("no_pobre", "pobre"),  # # Establece la referencia control y caso (empleado = negativo, desempleado = positivo) 
                  direction = "<")  # "<" significa que "desempleado" es positivo
     
prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))
prec_recall

prec_recall<- prec_recall  %>% mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

# Predicción de probabilidad de desempleo
test_RFPR<- test_final %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "prob")) 

# Clasificación según el umbral óptimo de F1
test_RFPR <- test_RFPR %>%
  mutate(pobre_hat_rangerPRs_cutoff_F1 = factor(
    ifelse(pobre_hat_ranger$pobre >= prec_recall$threshold[which.max(prec_recall$F1)], 1, 0),
    levels = c(1, 0),
    labels = c("pobre", "no_pobre")
  ))
probRF_1 <- ifelse (test_RFPR$pobre_hat_rangerPRs_cutoff_F1=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- prec_recall$threshold[which.max(prec_recall$F1)]

nombre_archivoRF <- sprintf(
  "RF_mtry_%s_min.node.size_%s_SR_%s_update.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))

```


```{r}
#############################################################################################################################
# RANDOM FOREST #5 (Optimizamos threshold con Precision recall Curve + imputación + nuevas variables + Variable Importance) #
#############################################################################################################################

#F1 = 0.70

var_imp=varImp(tree_ranger_grid)
df_vi=data.frame(var_imp$importance)
df_vi$importance <- as.integer(df_vi$Overall)
df_vi$feature <- rownames(df_vi)
df_vi <- df_vi[order(-df_vi$Overall), ]

write_xlsx(VI, here::here("stores", "VI_exportado.xlsx"))

#Grafico de barras importance para determinar threshold de importancia

df_vi$importance <- pmax(df_vi$importance, 0)

o  <- order(df_vi$importance, decreasing = TRUE)
y0 <- df_vi$importance[o]
feat <- df_vi$feature[o]

# Normaliza
y <- (y0 - min(y0)) / (max(y0) - min(y0))
x <- seq_along(y)

# Distancia a la línea entre (1, y[1]) y (n, y[n])
p1 <- c(1, y[1]); p2 <- c(length(y), y[length(y)])
num <- abs((p2[2]-p1[2])*x - (p2[1]-p1[1])*y + p2[1]*p1[2] - p2[2]*p1[1])
den <- sqrt((p2[2]-p1[2])^2 + (p2[1]-p1[1])^2)
dist <- num / den

knee_idx <- which.max(dist)
corte_importancia <- y0[knee_idx]
seleccion <- feat[seq_len(knee_idx)]

cat("Codo en índice", knee_idx, ":", feat[knee_idx], 
    " | importancia (sin normalizar):", corte_importancia, "\n")

ggplot(data.frame(x, y), aes(x, y)) +
  geom_line() + geom_point(size = 1) +
  geom_vline(xintercept = knee_idx, linetype = "dashed") +
  labs(x = "Rango (ordenado)", y = "Importancia normalizada",
       title = "Codo en Importancia de Variables (distancia máxima)")

# Filtrar df_vi
#Se definió 9 como threshold con el objetivo de reducir el universo de variables, pero manteniendo variables importantes estadísitcamente, pero también economicamente.
df_vi <- df_vi[df_vi$importance > 9, ]

# Crear vector de variables importantes
vars_imp <- df_vi$feature

# Para train_use
vars_imp_existentes <- vars_imp[vars_imp %in% names(train_use)]
# Para test_final
vars_imp_existentes_test <- vars_imp[vars_imp %in% names(test_final)]

# Seleccionar columnas en train_use
train_use <- train_use %>%
  dplyr::select(Pobre, all_of(vars_imp_existentes))

# Seleccionar columnas en test_final
test_final <- test_final %>%
  dplyr::select(all_of(vars_imp_existentes_test))

# ============= Preparar Paralelización ===============

# Detectar el número de núcleos disponibles
n_cores <- detectCores() - 4  # deja 1 libre para el sistema

# Crear el clúster
cl <- makeCluster(n_cores)

# Registrar el clúster para que caret lo use
registerDoParallel(cl)

# Confirmar
getDoParWorkers()

#=====================================================

ctrl <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = fiveStats, savePredictions = TRUE)

set.seed(2050)  # Fijamos semilla para reproducibilidad


p_load("ranger")
cat("Entrenando modelo con", getDoParWorkers(), "núcleos...\n")
start_time <- Sys.time()
tree_ranger_grid <-train (
    Pobre ~ .,  # Fórmula del modelo
    data = train_use,  # Dataset de entrenamiento
    method = "ranger",  # Usamos el motor ranger para Random Forests
    trControl = ctrl, # Especificamos los controles de entrenamiento
    verbose=TRUE,#Mirar progreso en el panel de control
    tuneGrid = expand.grid(   # Definimos la grilla de hiperparámetros a explorar
        mtry = c(5,7,9),  # Número de predictores seleccionados al azar en cada división
        splitrule = "gini",  # Regla de partición basada en la reducción de varianza (regresión)
        min.node.size = c(30, 50)  # Tamaño mínimo de nodos terminales
    ),
    metric = "F",  # Optimiza la métrica de F
    num.trees = 200 
)

tree_ranger_grid

p_load("pROC") # Paquete para calcular y visualizar curvas ROC

roc_obj_en<-roc(response=tree_ranger_grid$pred$obs,  # Valores reales de la variable objetivo
                   predictor=tree_ranger_grid$pred$pobre, # Probabilidades predichas por el modelo
                  levels = c("no_pobre", "pobre"),  # # Establece la referencia control y caso (empleado = negativo, desempleado = positivo) 
                  direction = "<")  # "<" significa que "desempleado" es positivo
     
prec_recall<-data.frame(coords(roc_obj_en, seq(0,1,length=100), ret=c("threshold", "precision", "recall")))
prec_recall

prec_recall<- prec_recall  %>% mutate(F1=(2*precision*recall)/(precision+recall))
prec_recall

prec_recall$threshold[which.max(prec_recall$F1)]

# Predicción de probabilidad de desempleo
test_RFPR<- test_final %>% mutate(pobre_hat_ranger=predict(tree_ranger_grid,newdata = test_final, type = "prob")) 

# Clasificación según el umbral óptimo de F1
test_RFPR <- test_RFPR %>%
  mutate(pobre_hat_rangerPRs_cutoff_F1 = factor(
    ifelse(pobre_hat_ranger$pobre >= prec_recall$threshold[which.max(prec_recall$F1)], 1, 0),
    levels = c(1, 0),
    labels = c("pobre", "no_pobre")
  ))
probRF_1 <- ifelse (test_RFPR$pobre_hat_rangerPRs_cutoff_F1=="no_pobre",0,1)


# ✅ Usa la misma fuente de IDs que funcionó con el LOGIT
id_out <- if ("id" %in% names(test_hogares_enriquecido)) {
  test_hogares_enriquecido$id
} else {
  seq_len(nrow(test_hogares_enriquecido))
}

predicciones_hogaresRF <- tibble(
  id = id_out,
  pobre_pred   = probRF_1
)

# Mantener consistencia con nombres y formato
best_mtry  <- tree_ranger_grid$bestTune$mtry
best_min.node.size <- tree_ranger_grid$bestTune$min.node.size
best_splitrule <- prec_recall$threshold[which.max(prec_recall$F1)]

nombre_archivoRF <- sprintf(
  "VI_RF_mtry_%s_min.node.size_%s_SR_%s_update.csv",
  format(best_mtry, 3),
  format(best_min.node.size, 2),
  format(best_splitrule, 2)  # para que quede trazable el cutoff
)

kaggle_sub <- predicciones_hogaresRF %>%
  transmute(
    id    = as.character(id),
    pobre = as.integer(pobre_pred)
  )

# Guardar
write_csv(kaggle_sub, here::here("stores", nombre_archivoRF))

```
